{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Process All Audio Files: Whisper + GPT-4o-mini Pipeline\n",
    "\n",
    "Process all 42 audio files through the validated Whisper + GPT-4o-mini pipeline.\n",
    "\n",
    "**Pipeline:**\n",
    "1. OpenAI Whisper API - Transcription (Kannada script)\n",
    "2. GPT-4o-mini - Transliteration (Roman script)\n",
    "3. Save individual JSON files with both versions\n",
    "4. Generate summary report\n",
    "\n",
    "**Features:**\n",
    "- Graceful error handling (continues on failures)\n",
    "- Progress tracking with time estimates\n",
    "- Incremental saves (no data loss on crashes)\n",
    "- Resume support (skip already-processed files)\n",
    "- Detailed failure reporting\n",
    "\n",
    "**Expected results:**\n",
    "- Individual JSON files in `files/transcriptions/batch_whisper_gpt4o/`\n",
    "- Summary CSV with costs, metrics, and status\n",
    "- Total cost: ~$3.27 for 9 hours of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from src.voice_eval.config import load_config\n",
    "from src.voice_eval.storage import list_files, write_file\n",
    "from src.voice_eval.whisper_api import transcribe_audio, transcription_to_dict, estimate_cost\n",
    "from src.voice_eval.gpt_transliteration import transliterate_text, transliterate_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment and configuration\n",
    "load_dotenv()\n",
    "language = load_config('whisper', 'language')\n",
    "audio_dir = load_config('input', 'audio_dir')\n",
    "dataset_name = load_config('dataset', 'name')\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Language: {language}\")\n",
    "print(f\"Audio directory: {audio_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing configuration\n",
    "OUTPUT_DIR = \"files/transcriptions/batch_whisper_gpt4o\"\n",
    "SKIP_EXISTING = True  # Set to False to reprocess all files\n",
    "SUPPORTED_EXTENSIONS = [\".mp3\", \".wav\"]  # Known working formats\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Skip existing: {SKIP_EXISTING}\")\n",
    "print(f\"Supported formats: {', '.join(SUPPORTED_EXTENSIONS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovery: Find All Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all audio files\n",
    "all_files = list_files(base_dir=audio_dir, pattern=\"*\")\n",
    "\n",
    "# Filter audio files (exclude .DS_Store, etc.)\n",
    "audio_extensions = [\".mp3\", \".mp4\", \".wav\", \".m4a\", \".aac\", \".amr\", \".webm\", \".mpeg\", \".mpga\"]\n",
    "audio_files = [f for f in all_files if Path(f).suffix.lower() in audio_extensions]\n",
    "\n",
    "print(f\"\\nFound {len(audio_files)} audio files to process:\")\n",
    "print(f\"  Total files in directory: {len(all_files)}\")\n",
    "print(f\"  Audio files: {len(audio_files)}\")\n",
    "print(f\"  Other files: {len(all_files) - len(audio_files)}\")\n",
    "\n",
    "# Show file format distribution\n",
    "format_counts = {}\n",
    "for f in audio_files:\n",
    "    ext = Path(f).suffix.lower()\n",
    "    format_counts[ext] = format_counts.get(ext, 0) + 1\n",
    "\n",
    "print(f\"\\nFile format distribution:\")\n",
    "for ext, count in sorted(format_counts.items()):\n",
    "    supported = \"✓\" if ext in SUPPORTED_EXTENSIONS else \"⚠️ \"\n",
    "    print(f\"  {supported} {ext}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(audio_path: str, output_dir: str) -> dict:\n",
    "    \"\"\"\n",
    "    Process a single audio file through the full pipeline.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - status: 'success' or 'failed'\n",
    "    - output_path: Path to saved JSON (if successful)\n",
    "    - error: Error message (if failed)\n",
    "    - metrics: Processing metrics (duration, cost, tokens, etc.)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    filename = Path(audio_path).name\n",
    "    \n",
    "    result = {\n",
    "        \"file\": filename,\n",
    "        \"status\": \"failed\",\n",
    "        \"output_path\": None,\n",
    "        \"error\": None,\n",
    "        \"processing_time_seconds\": 0,\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Transcribe with Whisper\n",
    "        print(f\"  [1/4] Transcribing with Whisper API...\")\n",
    "        transcript = transcribe_audio(\n",
    "            audio_path=audio_path,\n",
    "            language=language,\n",
    "            response_format=\"verbose_json\",\n",
    "            timestamp_granularities=[\"segment\"]\n",
    "        )\n",
    "        \n",
    "        whisper_cost = estimate_cost(transcript.duration)\n",
    "        print(f\"    ✓ Duration: {transcript.duration:.1f}s, Segments: {len(transcript.segments)}, Cost: ${whisper_cost:.4f}\")\n",
    "        \n",
    "        # Step 2: Transliterate full text\n",
    "        print(f\"  [2/4] Transliterating full text...\")\n",
    "        romanized_text, full_text_metrics = transliterate_text(\n",
    "            text=transcript.text,\n",
    "            source_language=\"Kannada\"\n",
    "        )\n",
    "        print(f\"    ✓ Cost: ${full_text_metrics['cost_usd']:.6f}\")\n",
    "        \n",
    "        # Step 3: Transliterate segments\n",
    "        print(f\"  [3/4] Transliterating {len(transcript.segments)} segments...\")\n",
    "        segments_dict = transcription_to_dict(transcript)[\"segments\"]\n",
    "        romanized_segments, segment_metrics = transliterate_segments(\n",
    "            segments=segments_dict,\n",
    "            text_field=\"text\",\n",
    "            source_language=\"Kannada\",\n",
    "            verbose=False  # Suppress per-segment progress\n",
    "        )\n",
    "        print(f\"    ✓ Cost: ${segment_metrics['total_cost_usd']:.6f}\")\n",
    "        \n",
    "        # Step 4: Save results\n",
    "        print(f\"  [4/4] Saving results...\")\n",
    "        total_cost = whisper_cost + segment_metrics['total_cost_usd']\n",
    "        \n",
    "        combined_response = {\n",
    "            \"metadata\": {\n",
    "                \"file\": filename,\n",
    "                \"file_path\": audio_path,\n",
    "                \"language\": transcript.language,\n",
    "                \"duration\": transcript.duration,\n",
    "                \"whisper_model\": \"whisper-1\",\n",
    "                \"transliteration_provider\": \"openai-gpt-4o-mini\",\n",
    "                \"processed_at\": datetime.now().isoformat(),\n",
    "                \"costs_usd\": {\n",
    "                    \"whisper_transcription\": whisper_cost,\n",
    "                    \"gpt_transliteration_full_text\": full_text_metrics['cost_usd'],\n",
    "                    \"gpt_transliteration_segments\": segment_metrics['total_cost_usd'],\n",
    "                    \"total_pipeline\": total_cost\n",
    "                },\n",
    "                \"metrics\": {\n",
    "                    \"total_tokens\": segment_metrics['total_tokens'],\n",
    "                    \"total_segments\": segment_metrics['total_segments'],\n",
    "                    \"transliteration_time_seconds\": segment_metrics['total_latency_ms'] / 1000\n",
    "                }\n",
    "            },\n",
    "            \"transcription\": {\n",
    "                \"text_kannada\": transcript.text,\n",
    "                \"text_romanized\": romanized_text\n",
    "            },\n",
    "            \"segments\": romanized_segments\n",
    "        }\n",
    "        \n",
    "        # Generate output filename (sanitize original filename)\n",
    "        safe_filename = Path(filename).stem.replace(\" \", \"_\").replace(\"+\", \"plus\")\n",
    "        output_filename = f\"{safe_filename}.json\"\n",
    "        \n",
    "        output_path = write_file(\n",
    "            output_filename,\n",
    "            json.dumps(combined_response, indent=2, ensure_ascii=False),\n",
    "            base_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"    ✓ Saved to: {output_path}\")\n",
    "        print(f\"    ✓ Total processing time: {processing_time:.1f}s\")\n",
    "        \n",
    "        result.update({\n",
    "            \"status\": \"success\",\n",
    "            \"output_path\": output_path,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"metrics\": {\n",
    "                \"duration\": transcript.duration,\n",
    "                \"segments\": len(transcript.segments),\n",
    "                \"whisper_cost\": whisper_cost,\n",
    "                \"gpt_cost\": segment_metrics['total_cost_usd'],\n",
    "                \"total_cost\": total_cost,\n",
    "                \"total_tokens\": segment_metrics['total_tokens']\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        error_msg = str(e)\n",
    "        print(f\"    ✗ Error: {error_msg}\")\n",
    "        \n",
    "        result.update({\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": error_msg,\n",
    "            \"processing_time_seconds\": processing_time\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def is_already_processed(audio_path: str, output_dir: str) -> bool:\n",
    "    \"\"\"Check if a file has already been processed.\"\"\"\n",
    "    filename = Path(audio_path).name\n",
    "    safe_filename = Path(filename).stem.replace(\" \", \"_\").replace(\"+\", \"plus\")\n",
    "    output_filename = f\"{safe_filename}.json\"\n",
    "    output_path = Path(output_dir) / output_filename\n",
    "    return output_path.exists()\n",
    "\n",
    "print(\"✓ Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing: Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STARTING BATCH PROCESSING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total files: {len(audio_files)}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Skip existing: {SKIP_EXISTING}\")\n",
    "print()\n",
    "\n",
    "# Track results\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, audio_file in enumerate(audio_files, 1):\n",
    "    filename = Path(audio_file).name\n",
    "    file_ext = Path(audio_file).suffix.lower()\n",
    "    \n",
    "    print(f\"\\n[{i}/{len(audio_files)}] Processing: {filename}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Check if already processed\n",
    "    if SKIP_EXISTING and is_already_processed(audio_file, OUTPUT_DIR):\n",
    "        print(f\"  ⏭️  Already processed (skipping)\")\n",
    "        results.append({\n",
    "            \"file\": filename,\n",
    "            \"status\": \"skipped\",\n",
    "            \"reason\": \"Already processed\"\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Warn if unsupported format\n",
    "    if file_ext not in SUPPORTED_EXTENSIONS:\n",
    "        print(f\"  ⚠️  Warning: {file_ext} format may not be supported by Whisper API\")\n",
    "        print(f\"     (Supported: {', '.join(SUPPORTED_EXTENSIONS)})\")\n",
    "        print(f\"     Attempting anyway...\")\n",
    "    \n",
    "    # Process file\n",
    "    result = process_single_file(audio_file, OUTPUT_DIR)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print progress summary\n",
    "    successful = sum(1 for r in results if r.get(\"status\") == \"success\")\n",
    "    failed = sum(1 for r in results if r.get(\"status\") == \"failed\")\n",
    "    skipped = sum(1 for r in results if r.get(\"status\") == \"skipped\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_file = elapsed_time / i if i > 0 else 0\n",
    "    remaining_files = len(audio_files) - i\n",
    "    estimated_remaining = avg_time_per_file * remaining_files\n",
    "    \n",
    "    print(f\"\\n  Progress: {i}/{len(audio_files)} ({i/len(audio_files)*100:.1f}%)\")\n",
    "    print(f\"  Status: ✓ {successful} success, ✗ {failed} failed, ⏭️  {skipped} skipped\")\n",
    "    print(f\"  Time: {elapsed_time/60:.1f}m elapsed, ~{estimated_remaining/60:.1f}m remaining\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "total_files = len(results)\n",
    "successful = [r for r in results if r.get(\"status\") == \"success\"]\n",
    "failed = [r for r in results if r.get(\"status\") == \"failed\"]\n",
    "skipped = [r for r in results if r.get(\"status\") == \"skipped\"]\n",
    "\n",
    "total_duration = sum(r.get(\"metrics\", {}).get(\"duration\", 0) for r in successful)\n",
    "total_cost = sum(r.get(\"metrics\", {}).get(\"total_cost\", 0) for r in successful)\n",
    "total_processing_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n📊 SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFiles Processed:\")\n",
    "print(f\"  ✓ Successful: {len(successful)} ({len(successful)/total_files*100:.1f}%)\")\n",
    "print(f\"  ✗ Failed:     {len(failed)} ({len(failed)/total_files*100:.1f}%)\")\n",
    "print(f\"  ⏭️  Skipped:    {len(skipped)} ({len(skipped)/total_files*100:.1f}%)\")\n",
    "print(f\"  📁 Total:      {total_files}\")\n",
    "\n",
    "if successful:\n",
    "    print(f\"\\nAudio Duration:\")\n",
    "    print(f\"  Total: {total_duration/3600:.2f} hours ({total_duration/60:.1f} minutes)\")\n",
    "    print(f\"  Average per file: {total_duration/len(successful)/60:.1f} minutes\")\n",
    "    \n",
    "    print(f\"\\nCosts:\")\n",
    "    print(f\"  Total: ${total_cost:.2f}\")\n",
    "    print(f\"  Average per file: ${total_cost/len(successful):.4f}\")\n",
    "    print(f\"  Per minute of audio: ${total_cost/(total_duration/60):.4f}\")\n",
    "    \n",
    "    print(f\"\\nProcessing Time:\")\n",
    "    print(f\"  Total: {total_processing_time/60:.1f} minutes ({total_processing_time/3600:.2f} hours)\")\n",
    "    print(f\"  Average per file: {total_processing_time/len(successful):.1f} seconds\")\n",
    "    print(f\"  Throughput: {total_duration/total_processing_time:.2f}x realtime\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\n❌ FAILED FILES ({len(failed)}):\")\n",
    "    print(\"-\" * 80)\n",
    "    for r in failed:\n",
    "        print(f\"  • {r['file']}\")\n",
    "        print(f\"    Error: {r.get('error', 'Unknown error')}\")\n",
    "        \n",
    "print(f\"\\n💾 Output Location: {OUTPUT_DIR}/\")\n",
    "print(f\"   Individual JSON files saved for each successful transcription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Summary CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results DataFrame\n",
    "summary_data = []\n",
    "for r in results:\n",
    "    row = {\n",
    "        \"file\": r[\"file\"],\n",
    "        \"status\": r[\"status\"],\n",
    "        \"duration_seconds\": r.get(\"metrics\", {}).get(\"duration\", None),\n",
    "        \"segments\": r.get(\"metrics\", {}).get(\"segments\", None),\n",
    "        \"whisper_cost_usd\": r.get(\"metrics\", {}).get(\"whisper_cost\", None),\n",
    "        \"gpt_cost_usd\": r.get(\"metrics\", {}).get(\"gpt_cost\", None),\n",
    "        \"total_cost_usd\": r.get(\"metrics\", {}).get(\"total_cost\", None),\n",
    "        \"total_tokens\": r.get(\"metrics\", {}).get(\"total_tokens\", None),\n",
    "        \"processing_time_seconds\": r.get(\"processing_time_seconds\", None),\n",
    "        \"output_path\": r.get(\"output_path\", None),\n",
    "        \"error\": r.get(\"error\", None),\n",
    "        \"reason\": r.get(\"reason\", None)\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save to CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "reports_dir = load_config('output', 'reports_dir')\n",
    "csv_path_timestamped = f\"{reports_dir}/batch_whisper_gpt4o_summary_{timestamp}.csv\"\n",
    "csv_path_latest = f\"{reports_dir}/batch_whisper_gpt4o_summary_latest.csv\"\n",
    "\n",
    "df.to_csv(csv_path_timestamped, index=False)\n",
    "df.to_csv(csv_path_latest, index=False)\n",
    "\n",
    "print(f\"\\n💾 Summary CSV saved:\")\n",
    "print(f\"  • {csv_path_timestamped}\")\n",
    "print(f\"  • {csv_path_latest}\")\n",
    "\n",
    "# Display summary table\n",
    "print(f\"\\n📋 Results Table (first 10 rows):\")\n",
    "print(df.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "✅ **Batch processing complete!**\n",
    "\n",
    "**If you had failed files due to format issues:**\n",
    "1. Review the failed files list above\n",
    "2. Convert unsupported formats (`.aac`, `.amr`, `.mp4`) to `.mp3` or `.wav`\n",
    "3. Re-run this notebook with `SKIP_EXISTING = True` to process only the failed files\n",
    "\n",
    "**Ready for next phase:**\n",
    "- Compare with Sarvam AI outputs\n",
    "- Compare with Azure Speech Services\n",
    "- Compare with AssemblyAI\n",
    "- Wait for professional transcriptions for WER/CER evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yi-voice-eval",
   "language": "python",
   "name": "yi-voice-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
