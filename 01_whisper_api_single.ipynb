{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Test OpenAI Whisper API on Single File\n",
    "\n",
    "Test the OpenAI Whisper API with one audio file to verify it works before batch processing.\n",
    "\n",
    "**Refactored to use:** `src/voice_eval/whisper_api.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from src.voice_eval.config import load_config\n",
    "from src.voice_eval.whisper_api import transcribe_audio, transcription_to_dict, estimate_cost\n",
    "from src.voice_eval.storage import write_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print(\"\u2713 Environment loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "language = load_config('whisper', 'language')\n",
    "audio_dir = load_config('input', 'audio_dir')\n",
    "\n",
    "print(f\"Language: {language}\")\n",
    "print(f\"Audio directory: {audio_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an MP3 file for testing\n",
    "test_file = f\"{audio_dir}/GHPS.  Bammanakatti.mp3\"\n",
    "print(f\"Test file: {test_file}\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "file_size_mb = os.path.getsize(test_file) / (1024 * 1024)\n",
    "print(f\"File size: {file_size_mb:.2f} MB (limit: 25 MB) \u2713\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe with OpenAI Whisper API\n",
    "print(\"\\nSending request to OpenAI Whisper API...\\n\")\n",
    "\n",
    "transcript = transcribe_audio(\n",
    "    audio_path=test_file,\n",
    "    language=language,\n",
    "    response_format=\"verbose_json\",\n",
    "    timestamp_granularities=[\"segment\"]\n",
    ")\n",
    "\n",
    "print(\"\u2713 Transcription complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METADATA:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Language: {transcript.language}\")\n",
    "print(f\"Duration: {transcript.duration:.2f} seconds\")\n",
    "print(f\"Cost estimate: ${estimate_cost(transcript.duration):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL TRANSCRIPTION:\")\n",
    "print(\"=\"*60)\n",
    "print(transcript.text[:500] + \"...\" if len(transcript.text) > 500 else transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display timestamped segments (first 3)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TIMESTAMPED SEGMENTS ({len(transcript.segments)} total - showing first 3):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, segment in enumerate(transcript.segments[:3], 1):\n",
    "    print(f\"\\n[Segment {i}]\")\n",
    "    print(f\"Time: {segment.start:.2f}s -> {segment.end:.2f}s\")\n",
    "    print(f\"Text: {segment.text}\")\n",
    "    print(f\"Confidence: no_speech_prob={segment.no_speech_prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response to JSON\n",
    "response_dict = transcription_to_dict(transcript)\n",
    "\n",
    "output_path = write_file(\n",
    "    \"whisper_api_test_response.json\",\n",
    "    json.dumps(response_dict, indent=2, ensure_ascii=False),\n",
    "    base_dir=\"files/transcriptions/whisper_api_test\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Response saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\u2705 Test successful!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yi-voice-eval)",
   "language": "python",
   "name": "yi-voice-eval"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}