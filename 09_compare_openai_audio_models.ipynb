{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare OpenAI Audio Models: Whisper vs GPT-4o vs GPT-4o-mini\n",
    "\n",
    "This notebook tests three OpenAI audio transcription models on a representative sample of files:\n",
    "\n",
    "1. **`whisper-1`** - Current baseline (Whisper Large v3)\n",
    "2. **`gpt-4o-audio-preview`** - Multimodal GPT-4o with native audio understanding\n",
    "3. **`gpt-4o-mini-audio-preview`** - Efficient multimodal model\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "GPT-4o's contextual reasoning may excel at:\n",
    "- Children's non-standard pronunciation\n",
    "- Math vocabulary in context\n",
    "- Kannada-English code-switching\n",
    "- Low-bandwidth audio artifacts\n",
    "\n",
    "## Test Set\n",
    "\n",
    "5 files representing different durations and complexity:\n",
    "- Ultra-short (10 sec)\n",
    "- Short (2.5 min)\n",
    "- Medium (9.6 min)\n",
    "- Long (16.2 min)\n",
    "- Very long (20.7 min)\n",
    "\n",
    "**Total**: ~49 minutes of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test files selected to represent different conditions\n",
    "TEST_FILES = [\n",
    "    \"+919742536994_3_converted.mp3\",           # Ultra-short: 10 sec, 1 segment\n",
    "    \"+919742536994_4_converted.mp3\",           # Short: 2.5 min, 2 segments\n",
    "    \"+917259326110_Fakkirswami S V_converted.mp3\",  # Medium: 9.6 min, 26 segments\n",
    "    \"+917259326110_Fakkirswami S V_2_converted.mp3\",  # Long: 16.2 min, 41 segments\n",
    "    \"GLPS AMBEDKAR NAGAR GUTTAL.mp3\",          # Very long: 20.7 min, 61 segments\n",
    "]\n",
    "\n",
    "# Models to test\n",
    "MODELS = [\n",
    "    \"whisper-1\",\n",
    "    \"gpt-4o-audio-preview\",\n",
    "    \"gpt-4o-mini-audio-preview\"\n",
    "]\n",
    "\n",
    "# Paths\n",
    "AUDIO_DIR = Path(\"../files/converted_audio\")\n",
    "OUTPUT_DIR = Path(\"../files/transcriptions/openai_model_comparison\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Test files: {len(TEST_FILES)}\")\n",
    "print(f\"Models to test: {MODELS}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_model(audio_path: Path, model: str) -> dict:\n",
    "    \"\"\"\n",
    "    Transcribe audio file using specified OpenAI model.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        model: Model name (whisper-1, gpt-4o-audio-preview, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing transcription result and metadata\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        with open(audio_path, 'rb') as audio_file:\n",
    "            # Call OpenAI Audio API\n",
    "            response = client.audio.transcriptions.create(\n",
    "                model=model,\n",
    "                file=audio_file,\n",
    "                language=\"kn\",  # Kannada\n",
    "                response_format=\"verbose_json\"\n",
    "            )\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Extract response data\n",
    "        result = {\n",
    "            'success': True,\n",
    "            'model': model,\n",
    "            'file': audio_path.name,\n",
    "            'text': response.text,\n",
    "            'language': response.language,\n",
    "            'duration': response.duration,\n",
    "            'segments': [{\n",
    "                'id': seg.id,\n",
    "                'start': seg.start,\n",
    "                'end': seg.end,\n",
    "                'text': seg.text,\n",
    "                'avg_logprob': seg.avg_logprob,\n",
    "                'no_speech_prob': seg.no_speech_prob\n",
    "            } for seg in response.segments] if hasattr(response, 'segments') else [],\n",
    "            'processing_time_seconds': processing_time,\n",
    "            'processed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        return {\n",
    "            'success': False,\n",
    "            'model': model,\n",
    "            'file': audio_path.name,\n",
    "            'error': str(e),\n",
    "            'processing_time_seconds': processing_time,\n",
    "            'processed_at': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "print(\"Transcription function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comparison Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in TEST_FILES:\n",
    "    audio_path = AUDIO_DIR / filename\n",
    "    \n",
    "    if not audio_path.exists():\n",
    "        print(f\"⚠️  File not found: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {filename}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    file_results = {'filename': filename, 'models': {}}\n",
    "    \n",
    "    for model in MODELS:\n",
    "        print(f\"\\n  Testing model: {model}...\")\n",
    "        \n",
    "        result = transcribe_with_model(audio_path, model)\n",
    "        file_results['models'][model] = result\n",
    "        \n",
    "        if result['success']:\n",
    "            duration = result.get('duration', 0)\n",
    "            processing_time = result['processing_time_seconds']\n",
    "            segments = len(result.get('segments', []))\n",
    "            \n",
    "            print(f\"    ✅ Success\")\n",
    "            print(f\"       Duration: {duration:.1f}s\")\n",
    "            print(f\"       Processing time: {processing_time:.1f}s ({processing_time/max(duration, 1):.2f}x realtime)\")\n",
    "            print(f\"       Segments: {segments}\")\n",
    "            print(f\"       Text preview: {result['text'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"    ❌ Failed: {result['error']}\")\n",
    "        \n",
    "        # Save individual result\n",
    "        output_file = OUTPUT_DIR / f\"{Path(filename).stem}_{model.replace('-', '_')}.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    results.append(file_results)\n",
    "    \n",
    "    # Small delay to avoid rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✅ Comparison test complete!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full comparison results\n",
    "summary_file = OUTPUT_DIR / f\"comparison_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract metrics for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for file_result in results:\n",
    "    filename = file_result['filename']\n",
    "    \n",
    "    for model, result in file_result['models'].items():\n",
    "        if result['success']:\n",
    "            comparison_data.append({\n",
    "                'filename': filename,\n",
    "                'model': model,\n",
    "                'duration_sec': result.get('duration', 0),\n",
    "                'processing_time_sec': result['processing_time_seconds'],\n",
    "                'realtime_factor': result['processing_time_seconds'] / max(result.get('duration', 1), 1),\n",
    "                'segment_count': len(result.get('segments', [])),\n",
    "                'avg_logprob': sum(s['avg_logprob'] for s in result.get('segments', [])) / max(len(result.get('segments', [])), 1),\n",
    "                'avg_no_speech_prob': sum(s['no_speech_prob'] for s in result.get('segments', [])) / max(len(result.get('segments', [])), 1),\n",
    "                'text_length': len(result['text'])\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nProcessing Speed (lower is faster):\")\n",
    "print(df.groupby('model')['realtime_factor'].mean().sort_values())\n",
    "\n",
    "print(\"\\nAverage Segments per File:\")\n",
    "print(df.groupby('model')['segment_count'].mean())\n",
    "\n",
    "print(\"\\nAverage Confidence (avg_logprob, higher is better):\")\n",
    "print(df.groupby('model')['avg_logprob'].mean())\n",
    "\n",
    "print(\"\\nAverage No-Speech Probability (lower suggests more actual speech):\")\n",
    "print(df.groupby('model')['avg_no_speech_prob'].mean())\n",
    "\n",
    "print(\"\\nAverage Text Length:\")\n",
    "print(df.groupby('model')['text_length'].mean())\n",
    "\n",
    "# Save detailed comparison\n",
    "csv_file = OUTPUT_DIR / f\"comparison_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"\\nDetailed metrics saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side Text Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display side-by-side transcriptions for manual quality review\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIDE-BY-SIDE TRANSCRIPTION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for file_result in results:\n",
    "    filename = file_result['filename']\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(f\"File: {filename}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for model in MODELS:\n",
    "        result = file_result['models'][model]\n",
    "        if result['success']:\n",
    "            print(f\"\\n{model}:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(result['text'][:500])  # First 500 chars\n",
    "            if len(result['text']) > 500:\n",
    "                print(\"[...]\")\n",
    "        else:\n",
    "            print(f\"\\n{model}: FAILED - {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook compared three OpenAI audio models:\n",
    "1. **whisper-1** - Purpose-built audio transcription\n",
    "2. **gpt-4o-audio-preview** - Multimodal reasoning with audio\n",
    "3. **gpt-4o-mini-audio-preview** - Efficient multimodal model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Manual Quality Review**: Read the side-by-side transcriptions above\n",
    "   - Which model handles children's voices best?\n",
    "   - Which correctly transcribes math vocabulary?\n",
    "   - Which handles Kannada-English code-switching best?\n",
    "\n",
    "2. **Decision Point**:\n",
    "   - If GPT-4o models significantly outperform: Consider using for full dataset\n",
    "   - If similar quality: Stick with whisper-1 (proven, cheaper)\n",
    "   - If mixed results: Use best model per file type\n",
    "\n",
    "3. **Cost-Benefit Analysis**:\n",
    "   - Calculate actual costs from processing\n",
    "   - Estimate full dataset costs for each model\n",
    "   - Weigh quality improvement vs. cost increase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yi-voice-eval",
   "language": "python",
   "name": "yi-voice-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
