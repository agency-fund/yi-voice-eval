{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Test OpenAI Whisper API on Single File\n",
    "\n",
    "Test the OpenAI Whisper API with one audio file to verify it works before batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from src.voice_eval.config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "print(\"\u2713 OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "language = load_config('whisper', 'language')\n",
    "audio_dir = load_config('input', 'audio_dir')\n",
    "\n",
    "print(f\"Language: {language}\")\n",
    "print(f\"Audio directory: {audio_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Pick an MP3 file for testing (explicitly supported format)\ntest_file = f\"{audio_dir}/GHPS.  Bammanakatti.mp3\"\nprint(f\"Test file: {test_file}\")\n\n# Check file size\nimport os\nfile_size_mb = os.path.getsize(test_file) / (1024 * 1024)\nprint(f\"File size: {file_size_mb:.2f} MB (limit: 25 MB) \u2713\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Transcribe with OpenAI Whisper API using verbose_json format\nprint(\"\\nSending request to OpenAI Whisper API...\\n\")\n\nwith open(test_file, 'rb') as audio_file:\n    transcript = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=language,\n        response_format=\"verbose_json\",\n        timestamp_granularities=[\"segment\"]\n    )\n\nprint(\"\u2713 Transcription complete!\")"
  },
  {
   "cell_type": "code",
   "id": "s56giboj4v",
   "source": "# Save raw response to JSON for inspection\nimport json\nfrom src.voice_eval.storage import write_file\n\n# Convert response to dict\nresponse_dict = {\n    \"text\": transcript.text,\n    \"language\": transcript.language,\n    \"duration\": transcript.duration,\n    \"segments\": [\n        {\n            \"id\": seg.id,\n            \"start\": seg.start,\n            \"end\": seg.end,\n            \"text\": seg.text,\n            \"tokens\": seg.tokens,\n            \"temperature\": seg.temperature,\n            \"avg_logprob\": seg.avg_logprob,\n            \"compression_ratio\": seg.compression_ratio,\n            \"no_speech_prob\": seg.no_speech_prob\n        }\n        for seg in transcript.segments\n    ]\n}\n\n# Save to JSON\noutput_path = write_file(\n    \"whisper_api_test_response.json\",\n    json.dumps(response_dict, indent=2, ensure_ascii=False),\n    base_dir=\"files/transcriptions/whisper_api_test\"\n)\n\nprint(f\"\u2713 Response saved to: {output_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METADATA:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Language: {transcript.language}\")\n",
    "print(f\"Duration: {transcript.duration:.2f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL TRANSCRIPTION:\")\n",
    "print(\"=\"*60)\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Display timestamped segments\nprint(\"\\n\" + \"=\"*60)\nprint(\"TIMESTAMPED SEGMENTS:\")\nprint(\"=\"*60)\n\nif hasattr(transcript, 'segments') and transcript.segments:\n    for i, segment in enumerate(transcript.segments, 1):\n        print(f\"\\n[Segment {i}]\")\n        print(f\"Time: {segment.start:.2f}s -> {segment.end:.2f}s\")\n        print(f\"Text: {segment.text}\")\n    print(f\"\\nTotal segments: {len(transcript.segments)}\")\nelse:\n    print(\"No segments available (may need different response_format)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show raw response structure for debugging\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAW RESPONSE STRUCTURE:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Type: {type(transcript)}\")\n",
    "print(f\"Available attributes: {dir(transcript)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yi-voice-eval)",
   "language": "python",
   "name": "yi-voice-eval"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}