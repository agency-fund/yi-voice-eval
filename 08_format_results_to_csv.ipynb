{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Batch Transcription Results to CSV\n",
    "\n",
    "This notebook converts JSON transcription results from the Whisper + GPT-4o-mini pipeline into CSV format for spreadsheet analysis.\n",
    "\n",
    "## Input\n",
    "- Folder containing JSON transcription files (default: `../files/transcriptions/batch_whisper_gpt4o/`)\n",
    "\n",
    "## Output\n",
    "Two CSV files saved to `../files/reports/`:\n",
    "\n",
    "1. **Summary CSV** (file-level): One row per audio file with full transcriptions and metadata\n",
    "   - Columns: filename, duration, language, full_kannada_text, full_romanized_text, segment_count, total_cost_usd, processed_at\n",
    "\n",
    "2. **Detailed CSV** (segment-level): One row per transcription segment with timestamps\n",
    "   - Columns: filename, segment_id, start_time, end_time, kannada_text, romanized_text, avg_logprob, no_speech_prob\n",
    "\n",
    "## Features\n",
    "- Configurable input folder path\n",
    "- Error handling for malformed JSON files\n",
    "- Progress tracking\n",
    "- Timestamped output files + \"latest\" versions\n",
    "- Preview of generated CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input folder containing JSON transcription files\n",
    "INPUT_FOLDER = Path(\"../files/transcriptions/batch_whisper_gpt4o\")\n",
    "\n",
    "# Output folder for CSV reports\n",
    "OUTPUT_FOLDER = Path(\"../files/reports\")\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Timestamp for output filenames\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"Input folder: {INPUT_FOLDER}\")\n",
    "print(f\"Output folder: {OUTPUT_FOLDER}\")\n",
    "print(f\"Timestamp: {TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files(folder_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load all JSON files from a folder.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing parsed JSON data\n",
    "    \"\"\"\n",
    "    json_files = list(folder_path.glob(\"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files in {folder_path}\")\n",
    "    \n",
    "    results = []\n",
    "    errors = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                results.append({\n",
    "                    'filename': json_file.name,\n",
    "                    'data': data\n",
    "                })\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading {json_file.name}: {str(e)}\"\n",
    "            errors.append(error_msg)\n",
    "            print(f\"âŒ {error_msg}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully loaded {len(results)} files\")\n",
    "    if errors:\n",
    "        print(f\"âš ï¸  {len(errors)} files failed to load\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all JSON files\n",
    "json_data = load_json_files(INPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summary CSV (File-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_csv(json_data: List[Dict[str, Any]], output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate summary CSV with one row per file.\n",
    "    \n",
    "    Args:\n",
    "        json_data: List of parsed JSON data\n",
    "        output_path: Path to save CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing summary data\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for item in json_data:\n",
    "        data = item['data']\n",
    "        metadata = data.get('metadata', {})\n",
    "        transcription = data.get('transcription', {})\n",
    "        segments = data.get('segments', [])\n",
    "        costs = metadata.get('costs_usd', {})\n",
    "        metrics = metadata.get('metrics', {})\n",
    "        \n",
    "        row = {\n",
    "            'filename': metadata.get('file', item['filename']),\n",
    "            'duration_seconds': metadata.get('duration', 0),\n",
    "            'language': metadata.get('language', 'unknown'),\n",
    "            'full_kannada_text': transcription.get('text_kannada', ''),\n",
    "            'full_romanized_text': transcription.get('text_romanized', ''),\n",
    "            'segment_count': len(segments),\n",
    "            'total_tokens': metrics.get('total_tokens', 0),\n",
    "            'whisper_cost_usd': costs.get('whisper_transcription', 0),\n",
    "            'gpt_cost_usd': costs.get('gpt_transliteration_segments', 0),\n",
    "            'total_cost_usd': costs.get('total_pipeline', 0),\n",
    "            'processed_at': metadata.get('processed_at', '')\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame and sort by filename\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values('filename').reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\nâœ… Summary CSV saved to: {output_path}\")\n",
    "    print(f\"   Rows: {len(df)}\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Calculate totals\n",
    "    total_duration = df['duration_seconds'].sum()\n",
    "    total_cost = df['total_cost_usd'].sum()\n",
    "    print(f\"\\nðŸ“Š Summary Statistics:\")\n",
    "    print(f\"   Total duration: {total_duration:.2f} seconds ({total_duration/3600:.2f} hours)\")\n",
    "    print(f\"   Total cost: ${total_cost:.4f}\")\n",
    "    print(f\"   Average cost per hour: ${(total_cost / (total_duration/3600)):.4f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate summary CSV with timestamp\n",
    "summary_output = OUTPUT_FOLDER / f\"batch_whisper_gpt4o_summary_{TIMESTAMP}.csv\"\n",
    "summary_df = generate_summary_csv(json_data, summary_output)\n",
    "\n",
    "# Also save as \"latest\" version for convenience\n",
    "summary_latest = OUTPUT_FOLDER / \"batch_whisper_gpt4o_summary_latest.csv\"\n",
    "summary_df.to_csv(summary_latest, index=False, encoding='utf-8')\n",
    "print(f\"\\nâœ… Also saved as: {summary_latest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Summary CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows (excluding full text columns for readability)\n",
    "preview_cols = ['filename', 'duration_seconds', 'language', 'segment_count', 'total_cost_usd', 'processed_at']\n",
    "print(\"\\nðŸ“‹ Summary CSV Preview (first 10 rows):\")\n",
    "print(summary_df[preview_cols].head(10).to_string(index=False))\n",
    "\n",
    "# Show sample of full text columns\n",
    "print(\"\\nðŸ“ Sample Full Text (first file):\")\n",
    "if len(summary_df) > 0:\n",
    "    print(f\"Kannada (first 100 chars): {summary_df.iloc[0]['full_kannada_text'][:100]}...\")\n",
    "    print(f\"Romanized (first 100 chars): {summary_df.iloc[0]['full_romanized_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Detailed CSV (Segment-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detailed_csv(json_data: List[Dict[str, Any]], output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate detailed CSV with one row per segment.\n",
    "    \n",
    "    Args:\n",
    "        json_data: List of parsed JSON data\n",
    "        output_path: Path to save CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing detailed segment data\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for item in json_data:\n",
    "        data = item['data']\n",
    "        metadata = data.get('metadata', {})\n",
    "        segments = data.get('segments', [])\n",
    "        filename = metadata.get('file', item['filename'])\n",
    "        \n",
    "        for segment in segments:\n",
    "            row = {\n",
    "                'filename': filename,\n",
    "                'segment_id': segment.get('id', ''),\n",
    "                'start_time': segment.get('start', 0),\n",
    "                'end_time': segment.get('end', 0),\n",
    "                'duration': segment.get('end', 0) - segment.get('start', 0),\n",
    "                'kannada_text': segment.get('text', ''),\n",
    "                'romanized_text': segment.get('text_romanized', ''),\n",
    "                'avg_logprob': segment.get('avg_logprob', None),\n",
    "                'no_speech_prob': segment.get('no_speech_prob', None)\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    # Create DataFrame and sort by filename and segment_id\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(['filename', 'segment_id']).reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\nâœ… Detailed CSV saved to: {output_path}\")\n",
    "    print(f\"   Rows: {len(df)} segments\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    print(f\"   Files covered: {df['filename'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate detailed CSV with timestamp\n",
    "detailed_output = OUTPUT_FOLDER / f\"batch_whisper_gpt4o_detailed_{TIMESTAMP}.csv\"\n",
    "detailed_df = generate_detailed_csv(json_data, detailed_output)\n",
    "\n",
    "# Also save as \"latest\" version for convenience\n",
    "detailed_latest = OUTPUT_FOLDER / \"batch_whisper_gpt4o_detailed_latest.csv\"\n",
    "detailed_df.to_csv(detailed_latest, index=False, encoding='utf-8')\n",
    "print(f\"\\nâœ… Also saved as: {detailed_latest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Detailed CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows (excluding full text columns for readability)\n",
    "preview_cols = ['filename', 'segment_id', 'start_time', 'end_time', 'duration', 'avg_logprob', 'no_speech_prob']\n",
    "print(\"\\nðŸ“‹ Detailed CSV Preview (first 10 segments):\")\n",
    "print(detailed_df[preview_cols].head(10).to_string(index=False))\n",
    "\n",
    "# Show sample of text columns\n",
    "print(\"\\nðŸ“ Sample Segment Text (first segment):\")\n",
    "if len(detailed_df) > 0:\n",
    "    print(f\"Kannada: {detailed_df.iloc[0]['kannada_text']}\")\n",
    "    print(f\"Romanized: {detailed_df.iloc[0]['romanized_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Successfully generated two CSV files:\n",
    "\n",
    "1. **Summary CSV**: File-level aggregation with full transcriptions\n",
    "2. **Detailed CSV**: Segment-level data with timestamps\n",
    "\n",
    "Both files are saved with timestamps and as \"latest\" versions for easy access.\n",
    "\n",
    "### Next Steps\n",
    "- Import CSVs into Google Sheets or Excel for analysis\n",
    "- Use detailed CSV for time-based analysis and quality checks\n",
    "- Use summary CSV for cost analysis and overview statistics\n",
    "- Filter segments by confidence scores (avg_logprob, no_speech_prob) to identify potential transcription issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
